# GDELTFlow: A Scalable Batch Data Pipeline ğŸš€

## ğŸ“Œ Overview
**GDELTFlow** is a **batch data processing pipeline** that ingests global news event data from **GDELT**, processes it using **Apache Spark on Dataproc**, and loads it into **BigQuery** for further analysis. The pipeline is orchestrated using **Kestra**, ensuring automation and scalability.

## ğŸ—ï¸ Architecture
```
GDELT Source â†’ Cloud Storage (Raw) â†’ Spark on Dataproc â†’ Cloud Storage (Processed) â†’ BigQuery â†’ dbt
```

### **Tech Stack**
âœ… [Kestra](https://kestra.io/) - Workflow Orchestration  
âœ… [Apache Spark](https://spark.apache.org/) - Data Processing  
âœ… [Google Cloud Storage (GCS)](https://cloud.google.com/storage/) - Data Lake  
âœ… [Google Dataproc](https://cloud.google.com/dataproc/) - Managed Spark Cluster
âœ… [Google BigQuery](https://cloud.google.com/bigquery/) - Data Warehouse  
âœ… [dbt](https://www.getdbt.com/) - Data Transformations

---

## âš™ï¸ Pipeline Workflow

1ï¸âƒ£ **Kestra fetches & stores daily GDELT data** in Google Cloud Storage.  
2ï¸âƒ£ **Spark on Dataproc cleans & transforms the data**, including:
   - Removing duplicates & handling missing values
   - Applying regex-based text cleaning on URLs & event descriptions
   - Partitioning by **date** and clustering by **event type**.

3ï¸âƒ£ **Kestra loads processed data into BigQuery**.

4ï¸âƒ£ **dbt performs final transformations** for analysis.

---

## ğŸ› ï¸ Setup & Deployment
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/yourusername/GDELTFlow.git
cd GDELTFlow
```

### **2ï¸âƒ£ Deploy Kestra Workflow**
Modify `kestra_gdelt_pipeline.yaml` with your GCS bucket & BQ dataset.
```bash
kestra deployment apply kestra_gdelt_pipeline.yaml
```

### **3ï¸âƒ£ Run Spark Job on Dataproc**
Submit Spark job for data processing:
```bash
gcloud dataproc jobs submit pyspark spark_jobs/gdelt_processing.py \
    --cluster=your-cluster-name --region=your-region \
    --properties=spark.executor.cores=4,spark.executor.memory=8g
```

### **4ï¸âƒ£ Load Data into BigQuery**
```bash
bq load --source_format=PARQUET your_project.dataset.gdelt_events \
    gs://your-bucket/processed/gdelt/*.parquet
```

### **5ï¸âƒ£ Run dbt Transformations**
```bash
dbt run
```

---

## ğŸ“Š Example Queries
Find top 10 most reported global events:
```sql
SELECT EventCode, COUNT(*) as event_count 
FROM `your_project.dataset.gdelt_events`
GROUP BY EventCode 
ORDER BY event_count DESC
LIMIT 10;
```

Find sentiment trends by country:
```sql
SELECT SQLDATE, ActionGeo_CountryCode, AVG(AvgTone) as avg_sentiment 
FROM `your_project.dataset.gdelt_events`
WHERE SQLDATE >= '2024-01-01'
GROUP BY SQLDATE, ActionGeo_CountryCode
ORDER BY SQLDATE DESC;
```

---

## ğŸš€ Future Improvements
ğŸ”¹ Add **real-time streaming** with Kafka + Spark Streaming  
ğŸ”¹ Implement **machine learning models** for event classification  
ğŸ”¹ Optimize Spark jobs using **Delta Lake**

---

## ğŸ† Contributors
- **[Khushal_Kharade](https://github.com/khushal2911)** - Data Enthusiast

Feel free to fork and contribute! âœ¨
